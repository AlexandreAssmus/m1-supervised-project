{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexical functions' id :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Oper1: ls:fr:lf:168\n",
    "Oper2: ls:fr:lf:190\n",
    "Oper3: ls:fr:lf:193\n",
    "Func0: ls:fr:lf:195\n",
    "Func1: ls:fr:lf:213\n",
    "Func2: ls:fr:lf:219\n",
    "Func3: ls:fr:lf:222\n",
    "Labor12: ls:fr:lf:224\n",
    "Labor13: ls:fr:lf:961\n",
    "Labor21: ls:fr:lf:225\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncurrent_dir = os.path.dirname(\"parsing_lf_verb.ipynb\")\\ncsv_path = os.path.join(current_dir, \\'..\\', \\'..\\', \\'lexical-system-fr\\', \\'ls-fr-V3\\', \\'15-lslf-rel.csv\\')\\ndf = pd.read_csv(csv_path, delimiter=\\'\\t\\')\\nfiltered_df = df[df[\\'lf\\'] == \\'ls:fr:lf:225\\'] # Oper / Func / Labor\\nresult = filtered_df[[\\'source\\', \\'target\\']]\\noutput_path = os.path.join(current_dir, \\'..\\', \\'result_Labor21.csv\\')\\nresult.to_csv(output_path, index=False)\\nprint(result)\\n\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "current_dir = os.path.dirname(\"parsing_lf_verb.ipynb\")\n",
    "csv_path = os.path.join(current_dir, '..', '..', 'lexical-system-fr', 'ls-fr-V3', '15-lslf-rel.csv')\n",
    "df = pd.read_csv(csv_path, delimiter='\\t')\n",
    "filtered_df = df[df['lf'] == 'ls:fr:lf:225'] # Oper / Func / Labor\n",
    "result = filtered_df[['source', 'target']]\n",
    "output_path = os.path.join(current_dir, '..', 'result_Labor21.csv')\n",
    "result.to_csv(output_path, index=False)\n",
    "print(result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Oper/Func/Labor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 source            target source_lexname target_lexname\n",
      "332    ls:fr:node:26269  ls:fr:node:50447       allergie       souffrir\n",
      "395    ls:fr:node:26291  ls:fr:node:34765            ami           être\n",
      "418    ls:fr:node:26292  ls:fr:node:56691          amour          avoir\n",
      "419    ls:fr:node:26292  ls:fr:node:27370          amour       éprouver\n",
      "420    ls:fr:node:26292  ls:fr:node:56763          amour         porter\n",
      "...                 ...               ...            ...            ...\n",
      "64600  ls:fr:node:56701  ls:fr:node:50447         ictère       souffrir\n",
      "64602  ls:fr:node:56702  ls:fr:node:56691     étonnement          avoir\n",
      "64603  ls:fr:node:56702  ls:fr:node:27370     étonnement       éprouver\n",
      "64604  ls:fr:node:56702  ls:fr:node:29223     étonnement      ressentir\n",
      "64686  ls:fr:node:56760  ls:fr:node:56763       jugement         porter\n",
      "\n",
      "[813 rows x 4 columns]\n",
      "Les données enrichies ont été sauvegardées dans ../support_verb/lf_source_target/Oper1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Etudiant\\AppData\\Local\\Temp\\ipykernel_18408\\3217328063.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result.loc[:, 'source_lexname'] = result['source'].map(id_to_lexname)\n",
      "C:\\Users\\Etudiant\\AppData\\Local\\Temp\\ipykernel_18408\\3217328063.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result.loc[:, 'target_lexname'] = result['target'].map(id_to_lexname)\n"
     ]
    }
   ],
   "source": [
    "# Load csv file \n",
    "df = pd.read_csv('../../lexical-system-fr/ls-fr-V3/15-lslf-rel.csv', delimiter='\\t')  \n",
    "nodes_df = pd.read_csv('../../lexical-system-fr/ls-fr-V3/01-lsnodes.csv', delimiter='\\t')\n",
    "\n",
    "# Filtrer les lignes où la colonne 'lf' est égale à 'id de la lf'\n",
    "filtered_df = df[df['lf'] == 'ls:fr:lf:168'] # Oper / Func / Labor\n",
    "\n",
    "# Sélectionner les colonnes 'source' et 'target'\n",
    "result = filtered_df[['source', 'target']]\n",
    "nodes_df['lexname_cleaned'] = nodes_df['lexname'].str.extract(r'>\\s*(.+?)\\s*<')\n",
    "id_to_lexname = pd.Series(nodes_df.lexname_cleaned.values, index=nodes_df.id).to_dict()\n",
    "# Appliquer les correspondances aux colonnes 'source' et 'target' de result\n",
    "result.loc[:, 'source_lexname'] = result['source'].map(id_to_lexname)\n",
    "result.loc[:, 'target_lexname'] = result['target'].map(id_to_lexname)\n",
    "#result.to_csv('../support_verb/result_Labor21.csv', index=False)\n",
    "# Afficher les résultats\n",
    "print(result)\n",
    "# Sauvegarder le résultat enrichi dans un nouveau fichier CSV\n",
    "output_file = '../support_verb/lf_source_target/Oper1.csv'\n",
    "result.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Data saved in \", output_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get lf files without duplicates in the target_lexname / source_lexname association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data without duplicates saved in ../support_verb/lf_source_target_clean/new_Oper3.csv\n"
     ]
    }
   ],
   "source": [
    "# Charger le fichier CSV\n",
    "new_df = pd.read_csv('../support_verb/lf_source_target/Oper3.csv')\n",
    "\n",
    "# Supprimer les doublons en se basant uniquement sur 'source_lexname' et 'target_lexname'\n",
    "unique_df = new_df.drop_duplicates(subset=['source_lexname', 'target_lexname'])\n",
    "\n",
    "# Trier les données par la colonne 'target_lexname'\n",
    "unique_df = unique_df.sort_values(by='target_lexname')\n",
    "\n",
    "# Sauvegarder le DataFrame sans doublons dans un nouveau fichier CSV\n",
    "output_file = '../support_verb/lf_source_target_clean/new_Oper3.csv'\n",
    "unique_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Data without duplicates saved in\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create combined file with all lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données combinées ont été sauvegardées dans ../support_verb/lf_source_target/combined_files.csv\n"
     ]
    }
   ],
   "source": [
    "# Chemin vers le dossier contenant les fichiers CSV\n",
    "directory = '../support_verb/lf_source_target/'\n",
    "\n",
    "# Liste pour stocker les DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Itérer sur chaque fichier dans le dossier spécifié\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Charger le fichier CSV\n",
    "        df = pd.read_csv(os.path.join(directory, filename))\n",
    "        \n",
    "        # Supprimer l'extension '.csv' du nom du fichier pour la colonne file_origin\n",
    "        filename_without_extension = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Ajouter une colonne indiquant le nom du fichier original sans extension\n",
    "        df['lexical_function'] = filename_without_extension\n",
    "        \n",
    "        # Ajouter le DataFrame à la liste\n",
    "        dfs.append(df)\n",
    "\n",
    "# Combiner tous les DataFrames dans un seul DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Sauvegarder le DataFrame combiné dans un nouveau fichier CSV\n",
    "output_file = '../support_verb/lf_source_target/combined_files.csv'\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Les données combinées ont été sauvegardées dans\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create combined files without duplicates in the target_lexname / source_lexname association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data without duplicates saved in ../support_verb/lf_source_target_clean/unique_associations.csv\n"
     ]
    }
   ],
   "source": [
    "# Chemin vers le fichier CSV combiné\n",
    "input_file = '../support_verb/lf_source_target/combined_files.csv'\n",
    "\n",
    "# Charger le DataFrame\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Supprimer les doublons en se basant uniquement sur 'source_lexname' et 'target_lexname'\n",
    "unique_df = df.drop_duplicates(subset=['source_lexname', 'target_lexname'])\n",
    "\n",
    "# Trier les données par la colonne 'target_lexname'\n",
    "unique_df = unique_df.sort_values(by='target_lexname')\n",
    "# Sauvegarder le DataFrame sans doublons dans un nouveau fichier CSV\n",
    "output_file = '../support_verb/lf_source_target_clean/unique_associations.csv'\n",
    "unique_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Data without duplicates saved in\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Oper3\n",
    "    haine, inspirer : \"Sa trahison inspire une profonde haine.\"\n",
    "    haine, provoquer : \"Leur injustice provoque une haine intense.\"\n",
    "    haine, susciter : \"Son comportement égoïste suscite de la haine.\"\n",
    "    félicitations, valoir : \"Ces félicitations valent plus que n'importe quel prix.\"\n",
    "    pluie, recevoir : \"La terre aride reçoit la pluie avec gratitude.\"\n",
    "    pluie, essuyer : \"Nous essuyons une forte pluie.\"\n",
    "    pluie, affronter : \"Ils affrontent une pluie incessante lors de leur voyage.\"\n",
    "    tempête, subir : \"La côte subit une tempête dévastatrice.\"\n",
    "    tempête, essuyer : \"La région essuie sa pire tempête en une décennie.\"\n",
    "    carton rouge, valoir : \"Ce carton rouge vaut une suspension du prochain match.\"\n",
    "    bilan, passer : \"Le directeur passe le bilan de l'année écoulée.\"\n",
    "    haine, inspirer : \"Ses paroles inspirent une haine inattendue.\"\n",
    "    haine, provoquer : \"Le débat provoque plus de haine que de résolution.\"\n",
    "    haine, susciter : \"L'annonce suscite une haine virulente parmi les fans.\"  \n",
    "\"\"\"\n",
    "\n",
    "\"\"\" \n",
    "Oper2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Etudiant\\M1_TAL\\m1-supervised-project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "If you want to use `CamembertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Écrire une phrase utilisant le nom 'haine' et le verbe 'inspirer':\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, CamembertTokenizer, CamembertModel\n",
    "\n",
    "def generate_text(source, verb):\n",
    "    # Initialiser le tokenizer et le modèle pour CamemBERT\n",
    "    tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "    model = CamembertModel.from_pretrained('camembert-base')\n",
    "    \n",
    "    # Charger le pipeline de génération de texte en utilisant CamemBERT\n",
    "    generator = pipeline('text-generation', model='camembert-base')\n",
    "\n",
    "    # Créer un prompt avec le nom et le verbe\n",
    "    prompt = f\"Écrire une phrase utilisant le nom '{source}' et le verbe '{verb}':\"\n",
    "\n",
    "    # Générer la réponse à partir du prompt en utilisant les tokens d'EOS comme pad_token_id\n",
    "    responses = generator(prompt, max_length=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Prendre la première réponse générée\n",
    "    generated_text = responses[0]['generated_text']\n",
    "    return generated_text.strip()\n",
    "\n",
    "# Exemple d'utilisation\n",
    "source_lexname = \"haine\"\n",
    "target_lexname = \"inspirer\"\n",
    "sentence = generate_text(source_lexname, target_lexname)\n",
    "print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Écrire une phrase en utilisant le nom 'haine' et le verbe 'inspirer':\n",
      "\n",
      "\"Clementine and his family were killed in a battle of the village of L'Aure. A certain prince was captured and forced into marriage with his mistress. But the princess died before she could bring forth her child, and the prince fled to France.\"\n",
      "\n",
      "Het d'une qui vérieux précul. Le vél\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "\n",
    "def generate_text(source, verb):\n",
    "    # Charger le tokenizer et le modèle GPT-2\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "    # Créer un pipeline de génération de texte avec GPT-2\n",
    "    text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Créer un prompt avec le nom et le verbe\n",
    "    prompt = f\"Écrire une phrase en utilisant le nom '{source}' et le verbe '{verb}':\"\n",
    "\n",
    "    # Générer la réponse à partir du prompt\n",
    "    responses = text_generator(prompt, max_length=100, num_return_sequences=1)\n",
    "\n",
    "    # Prendre la première réponse générée\n",
    "    generated_text = responses[0]['generated_text']\n",
    "    return generated_text.strip()\n",
    "\n",
    "# Exemple d'utilisation\n",
    "source_lexname = \"haine\"\n",
    "target_lexname = \"inspirer\"\n",
    "sentence = generate_text(source_lexname, target_lexname)\n",
    "print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nOper2\\n\\n\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
